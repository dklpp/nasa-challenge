"""
TOI Classification with Training Curves Analysis
Records and visualizes training/validation curves for loss and accuracy
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split, validation_curve, learning_curve
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, log_loss
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class TOIModelWithCurves:
    def __init__(self, data_path="/Users/lilianahotsko/Desktop/nasa-challenge/data/clean/TOI_2025.10.04_08.44.51.csv"):
        self.data_path = data_path
        self.scaler = RobustScaler()
        self.label_encoder = None
        self.training_history = {}
        
    def load_and_preprocess_data(self):
        """Load and preprocess data with key features"""
        print("üîç LOADING AND PREPROCESSING DATA")
        print("="*50)
        
        df = pd.read_csv(self.data_path)
        
        # Remove identifiers
        id_cols = ['toi', 'tid', 'rastr', 'decstr', 'toi_created', 'rowupdate']
        df = df.drop(columns=[col for col in id_cols if col in df.columns])
        
        # Handle missing values
        error_cols = [col for col in df.columns if 'err' in col.lower()]
        limit_cols = [col for col in df.columns if 'lim' in col.lower()]
        
        for col in error_cols + limit_cols:
            df[col] = df[col].fillna(0)
        
        measurement_cols = df.select_dtypes(include=[np.number]).columns
        measurement_cols = [col for col in measurement_cols 
                          if col not in error_cols and col not in limit_cols]
        
        for col in measurement_cols:
            if df[col].isnull().sum() > 0:
                group_medians = df.groupby('tfopwg_disp')[col].median()
                df[col] = df[col].fillna(df['tfopwg_disp'].map(group_medians))
                df[col] = df[col].fillna(df[col].median())
        
        # Key feature engineering
        print("Creating key astrophysical features...")
        
        if 'pl_rade' in df.columns and 'pl_orbper' in df.columns:
            df['pl_density_proxy'] = df['pl_rade'] / (df['pl_orbper'] ** (2/3))
            
        if 'pl_insol' in df.columns:
            df['pl_insol_log'] = np.log10(df['pl_insol'] + 1e-8)
            df['habitable_zone'] = ((df['pl_insol'] >= 0.5) & (df['pl_insol'] <= 2.0)).astype(int)
            
        if 'pl_eqt' in df.columns:
            df['earth_like_temp'] = ((df['pl_eqt'] >= 200) & (df['pl_eqt'] <= 350)).astype(int)
            
        if 'st_teff' in df.columns and 'st_rad' in df.columns:
            df['st_luminosity'] = (df['st_rad'] ** 2) * ((df['st_teff'] / 5778) ** 4)
            
        if 'st_tmag' in df.columns:
            df['st_observable'] = (df['st_tmag'] < 12).astype(int)
            
        print(f"Enhanced features: {df.shape[1]}")
        return df
    
    def prepare_data(self, df):
        """Prepare data for training with curves tracking"""
        print("\nüìä PREPARING DATA")
        print("="*50)
        
        # Encode target
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(df['tfopwg_disp'])
        X = df.drop(['tfopwg_disp'], axis=1)
        
        # Handle categorical variables
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns
        for col in categorical_cols:
            le_col = LabelEncoder()
            X[col] = le_col.fit_transform(X[col].astype(str))
        
        X = X.fillna(X.median())
        
        print(f"Data shape: {X.shape}")
        print(f"Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # Train-validation-test split
        X_temp, X_test, y_temp, y_test = train_test_split(
            X.values, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 total
        )
        
        print(f"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")
        
        # Scale data
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Apply resampling only to training data
        print("\n‚öñÔ∏è APPLYING RESAMPLING")
        print("="*30)
        
        sampler = SMOTETomek(random_state=42)
        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_scaled, y_train)
        
        print(f"Resampled training: {dict(zip(*np.unique(y_train_resampled, return_counts=True)))}")
        
        return X_train_resampled, X_val_scaled, X_test_scaled, y_train_resampled, y_val, y_test
    
    def train_xgboost_with_curves(self, X_train, X_val, y_train, y_val):
        """Train XGBoost with detailed training curves"""
        print("\nüöÄ TRAINING XGBOOST WITH CURVES")
        print("="*50)
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=1,
            reg_lambda=1,
            random_state=42,
            eval_metric='mlogloss',
            early_stopping_rounds=50
        )
        
        # Train with evaluation sets to track curves
        model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train), (X_val, y_val)],
            verbose=False
        )
        
        # Extract training history
        results = model.evals_result()
        eval_keys = list(results.keys())
        
        self.training_history['xgboost'] = {
            'train_loss': results[eval_keys[0]]['mlogloss'],
            'val_loss': results[eval_keys[1]]['mlogloss'],
            'n_estimators': len(results[eval_keys[0]]['mlogloss'])
        }
        
        # Calculate accuracy curves
        train_accuracies = []
        val_accuracies = []
        
        # Use incremental training to get accuracy at each step
        for n_est in range(50, model.n_estimators + 1, 50):
            temp_model = xgb.XGBClassifier(
                n_estimators=n_est,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=1,
                reg_lambda=1,
                random_state=42,
                eval_metric='mlogloss'
            )
            temp_model.fit(X_train, y_train, verbose=False)
            
            train_acc = accuracy_score(y_train, temp_model.predict(X_train))
            val_acc = accuracy_score(y_val, temp_model.predict(X_val))
            
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)
        
        self.training_history['xgboost']['train_accuracy'] = train_accuracies
        self.training_history['xgboost']['val_accuracy'] = val_accuracies
        self.training_history['xgboost']['accuracy_steps'] = list(range(50, model.n_estimators + 1, 50))
        
        print(f"XGBoost trained with {model.n_estimators} estimators")
        return model
    
    def train_lightgbm_with_curves(self, X_train, X_val, y_train, y_val):
        """Train LightGBM with detailed training curves"""
        print("\nüöÄ TRAINING LIGHTGBM WITH CURVES")
        print("="*50)
        
        # Prepare datasets for LightGBM
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        params = {
            'objective': 'multiclass',
            'num_class': len(np.unique(y_train)),
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 1,
            'reg_lambda': 1,
            'random_state': 42,
            'verbose': -1
        }
        
        # Train with callbacks to record training curves
        callbacks = []
        eval_results = {}
        
        model = lgb.train(
            params,
            train_data,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'val'],
            num_boost_round=500,
            callbacks=[lgb.record_evaluation(eval_results), lgb.early_stopping(50)]
        )
        
        # Store training history
        self.training_history['lightgbm'] = {
            'train_loss': eval_results['train']['multi_logloss'],
            'val_loss': eval_results['val']['multi_logloss'],
            'n_estimators': len(eval_results['train']['multi_logloss'])
        }
        
        # Calculate accuracy curves for LightGBM
        train_accuracies = []
        val_accuracies = []
        
        for i in range(0, model.num_trees(), 50):
            train_pred = model.predict(X_train, num_iteration=i+50)
            val_pred = model.predict(X_val, num_iteration=i+50)
            
            train_pred_class = np.argmax(train_pred, axis=1)
            val_pred_class = np.argmax(val_pred, axis=1)
            
            train_acc = accuracy_score(y_train, train_pred_class)
            val_acc = accuracy_score(y_val, val_pred_class)
            
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)
        
        self.training_history['lightgbm']['train_accuracy'] = train_accuracies
        self.training_history['lightgbm']['val_accuracy'] = val_accuracies
        self.training_history['lightgbm']['accuracy_steps'] = list(range(50, model.num_trees() + 1, 50))
        
        print(f"LightGBM trained with {model.num_trees()} trees")
        return model
    
    def plot_training_curves(self):
        """Plot comprehensive training curves"""
        print("\nüìà PLOTTING TRAINING CURVES")
        print("="*50)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Training Curves Analysis', fontsize=16, fontweight='bold')
        
        # XGBoost Loss Curves
        if 'xgboost' in self.training_history:
            xgb_history = self.training_history['xgboost']
            epochs = range(1, len(xgb_history['train_loss']) + 1)
            
            axes[0, 0].plot(epochs, xgb_history['train_loss'], 'b-', label='Training Loss', linewidth=2)
            axes[0, 0].plot(epochs, xgb_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
            axes[0, 0].set_title('XGBoost - Loss Curves')
            axes[0, 0].set_xlabel('Boosting Rounds')
            axes[0, 0].set_ylabel('Log Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # XGBoost Accuracy Curves
            if 'train_accuracy' in xgb_history:
                axes[0, 1].plot(xgb_history['accuracy_steps'], xgb_history['train_accuracy'], 
                              'b-', label='Training Accuracy', linewidth=2)
                axes[0, 1].plot(xgb_history['accuracy_steps'], xgb_history['val_accuracy'], 
                              'r-', label='Validation Accuracy', linewidth=2)
                axes[0, 1].set_title('XGBoost - Accuracy Curves')
                axes[0, 1].set_xlabel('Boosting Rounds')
                axes[0, 1].set_ylabel('Accuracy')
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)
        
        # LightGBM Loss Curves
        if 'lightgbm' in self.training_history:
            lgb_history = self.training_history['lightgbm']
            epochs = range(1, len(lgb_history['train_loss']) + 1)
            
            axes[1, 0].plot(epochs, lgb_history['train_loss'], 'g-', label='Training Loss', linewidth=2)
            axes[1, 0].plot(epochs, lgb_history['val_loss'], 'orange', label='Validation Loss', linewidth=2)
            axes[1, 0].set_title('LightGBM - Loss Curves')
            axes[1, 0].set_xlabel('Boosting Rounds')
            axes[1, 0].set_ylabel('Log Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # LightGBM Accuracy Curves
            if 'train_accuracy' in lgb_history:
                axes[1, 1].plot(lgb_history['accuracy_steps'], lgb_history['train_accuracy'], 
                              'g-', label='Training Accuracy', linewidth=2)
                axes[1, 1].plot(lgb_history['accuracy_steps'], lgb_history['val_accuracy'], 
                              'orange', label='Validation Accuracy', linewidth=2)
                axes[1, 1].set_title('LightGBM - Accuracy Curves')
                axes[1, 1].set_xlabel('Boosting Rounds')
                axes[1, 1].set_ylabel('Accuracy')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/Users/lilianahotsko/Desktop/nasa-challenge/ml_tabular/training_curves.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # Additional analysis plots
        self.plot_learning_curves()
    
    def plot_learning_curves(self):
        """Plot learning curves to analyze model performance vs training size"""
        print("\nüìä PLOTTING LEARNING CURVES")
        print("="*50)
        
        # This will be implemented with a simple model for demonstration
        from sklearn.ensemble import RandomForestClassifier
        
        # Use a subset of data for learning curves (computationally expensive)
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        # We'll create a simple version - in practice you'd use the full pipeline
        print("Learning curves analysis completed (simplified version)")
    
    def analyze_training_behavior(self):
        """Analyze training behavior from curves"""
        print("\nüîç TRAINING BEHAVIOR ANALYSIS")
        print("="*50)
        
        for model_name, history in self.training_history.items():
            print(f"\n{model_name.upper()} Analysis:")
            print("-" * 30)
            
            train_loss = history['train_loss']
            val_loss = history['val_loss']
            
            # Overfitting analysis
            final_train_loss = train_loss[-1]
            final_val_loss = val_loss[-1]
            loss_gap = final_val_loss - final_train_loss
            
            print(f"Final Training Loss: {final_train_loss:.4f}")
            print(f"Final Validation Loss: {final_val_loss:.4f}")
            print(f"Loss Gap (Val - Train): {loss_gap:.4f}")
            
            if loss_gap > 0.1:
                print("‚ö†Ô∏è  Potential overfitting detected")
            elif loss_gap > 0.05:
                print("üìä Mild overfitting")
            else:
                print("‚úÖ Good generalization")
            
            # Convergence analysis
            if len(train_loss) >= 10:
                recent_improvement = train_loss[-10] - train_loss[-1]
                if recent_improvement < 0.001:
                    print("üìà Model converged")
                else:
                    print("üîÑ Model still improving")
            
            # Best validation performance
            best_val_idx = np.argmin(val_loss)
            best_val_loss = val_loss[best_val_idx]
            print(f"Best Validation Loss: {best_val_loss:.4f} at round {best_val_idx + 1}")
    
    def run_training_with_curves(self):
        """Run complete training pipeline with curve tracking"""
        print("üèÜ TOI TRAINING WITH CURVES ANALYSIS")
        print("="*80)
        
        # 1. Load and preprocess
        df = self.load_and_preprocess_data()
        
        # 2. Prepare data
        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data(df)
        
        # 3. Train models with curve tracking
        xgb_model = self.train_xgboost_with_curves(X_train, X_val, y_train, y_val)
        lgb_model = self.train_lightgbm_with_curves(X_train, X_val, y_train, y_val)
        
        # 4. Plot training curves
        self.plot_training_curves()
        
        # 5. Analyze training behavior
        self.analyze_training_behavior()
        
        # 6. Final evaluation
        print("\nüìä FINAL MODEL EVALUATION")
        print("="*50)
        
        # Test XGBoost
        xgb_pred = xgb_model.predict(X_test)
        xgb_accuracy = accuracy_score(y_test, xgb_pred)
        
        # Test LightGBM
        lgb_pred_proba = lgb_model.predict(X_test)
        lgb_pred = np.argmax(lgb_pred_proba, axis=1)
        lgb_accuracy = accuracy_score(y_test, lgb_pred)
        
        print(f"XGBoost Test Accuracy: {xgb_accuracy:.4f}")
        print(f"LightGBM Test Accuracy: {lgb_accuracy:.4f}")
        
        # Choose best model
        if xgb_accuracy > lgb_accuracy:
            best_model = xgb_model
            best_pred = xgb_pred
            best_accuracy = xgb_accuracy
            best_name = "XGBoost"
        else:
            best_model = lgb_model
            best_pred = lgb_pred
            best_accuracy = lgb_accuracy
            best_name = "LightGBM"
        
        print(f"\nüèÜ Best Model: {best_name} with {best_accuracy:.4f} accuracy")
        
        # Detailed report for best model
        target_names = self.label_encoder.classes_
        print(f"\nDetailed Classification Report ({best_name}):")
        print(classification_report(y_test, best_pred, target_names=target_names))
        
        return best_model, best_accuracy, self.training_history

def main():
    """Run the training with curves analysis"""
    model = TOIModelWithCurves()
    best_model, accuracy, history = model.run_training_with_curves()
    return model, best_model, accuracy, history

if __name__ == "__main__":
    model, best_model, accuracy, history = main()
